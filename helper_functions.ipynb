{"cells":[{"cell_type":"code","source":["## Phases Run:\n#### Setup Environment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540dd881-e742-40ab-b280-75ff48ceb3f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import time\nimport random\nimport numpy as np\nimport pandas as pd\nimport airportsdata\nfrom itertools import chain\nfrom datetime import datetime \nimport matplotlib.pyplot as plt\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import Row, Column\nfrom pyspark.ml.feature import Imputer\nfrom pyspark.sql.window import Window\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.mllib.linalg import SparseVector, DenseVector\nfrom pyspark.sql.types import BooleanType, StringType, IntegerType, DoubleType, LongType\nfrom pyspark.ml.feature import OneHotEncoder, StandardScaler\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\n# custom configuration\nsc = spark.sparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"803c1931-9f05-4835-ad58-9e9081041168"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# PIPELINE DEVELOPMENT\n# If you would like to update any parts of the pipeline individually, using the get_ commands with write = True is the recommended method. \n# Airlines - get_airlines(write=True)\n# Stations - get_stations(write=True)\n# Individual stations - get_stations_agg(write=True)\n# Airline & station join - get_flights(df_airlines_post, df_stations_agg, env='dev', write=True)\n# Weather - get_weather(weather_filters, stations_list, flightMonths_list, flightYears_list, env='dev', write=True)\n# Anything after airline and station join, usage for updating is can be found in the process_pipeline function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836d3ee4-ad26-4d23-bd2d-e4efdb1fd092"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["blob_container = \"w261\" # The name of your container created in https://portal.azure.com\nstorage_account = \"w261finalprojectv2\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261-scope\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"w261-key-2\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"860d1e18-8843-40ec-9b06-baca76a0b902"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\n  f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1032131-e1af-4dcf-9f6a-02f9a78f6bb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Configurations\nmax_write_rows = 200000"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d80ab7f-54a2-4288-9d16-1d1d8a7b50ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Airlines Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"267b5d23-4838-42a9-b12a-2cbab83d9e4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# *Spencer's Notes*\n# * Output Columns\n#   * DIV *: As best as I can tell, we have a series of equivalent data for diverted flights\n#   * DEL *: Really, *DEL* (DEP_DELAY, ARR_DELAY, etc.)\n#   * TAXI_OUT: Time elapsed between departed from the airport gate and wheels off the ground\n#   * WHEELS_OFF: Take-off time from departure airport\n#   * WHEELS_ON: Landing time at arrival airport (Add to `delay_cols`?)\n#   * TAXI-IN: Time between landing and arrival at airport gate (Add to `delay_cols`?)\n#   * CRS_ARR_TIME: Computer Reservation System \n#     * CRS INFO: https://www.bts.gov/topics/airlines-and-airports/number-14-time-reporting\n#     * CRS_ARR_TIME = scheduled arrival time, CRS_ELAPSED_TIME = scheduled elapsed time,etc. \n#     * Some of this info might be useful. For example, flights expected to be shorter or longer, as specified by CRS_ELAPSED_TIME, might be delayed with different probabilities. \n#   * CANCELLATION_CODE: A = Carrier, B = Weather, C= National Air System, D = Security\n#     * [Source](https://www.transtats.bts.gov/FieldInfo.asp?Svryq_Qr5p=f2rpvsvr5%FDgur%FDern510%FDS14%FDPn0pryyn6v10&Svryq_gB2r=Pun4&Y11x72_gnoyr=Y_PNaPRYYNgVba&gnoyr_VQ=FGJ&flf_gnoyr_anzr=g_bagVZR_eRcbegVaT&fB5_Svryq_anzr=PNaPRYYNgVba_PbQR)\n#     * Knowing if a prior flight from an airport is cancelled might be informative\n#   * TOTAL ADD GTIM: Total Ground Time Away from Gate for Gate Return or Cancelled Flight\n#   * LONGEST AGG GTIME: Longest Time Away from Gate for Gate Return or Cancelled Flight\n# * With a lot of these variables, I believe some clever (yet hefty) feature engineering could create some interesting variables. However, such feature engineering does not obviously (at least to me) pass a cost benefit analysis, so we can evaluate rather we need more informative variables later on. \n# * What percantage of flights are diverted? How do we categorize these?\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5c1917e-5ff8-4b12-8270-946d19b165a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# airline data dictionary - https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ\n\n# OUTPUT COLUMNS - related to delays or come after the fact for a flight\n# # 'DIV*', 'DEL*', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_TIME_BLK', 'CANCELLATION_CODE', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'TOTAL_ADD_GTIME', 'LONGEST_ADD_GTIME', 'DIV_ACTUAL_ELAPSED_TIME', 'DIV_REACHED_DEST'\n\n# DUPLICATIVE\n# OP_CARRIER_AIRLINE_ID - OP_UNIQUE_CARRIER & OP_CARRIER \n# ORIGIN_AIRPORT_ID - ORIGIN,  ORIGIN_CITY_NAME, ORIGIN_CITY_MARKET_ID,  ORIGIN_STATE_ABR, ORIGIN_STATE_NM, ORIGIN_AIRPORT_SEQ_ID\n# DEST_AIRPORT_SEQ_ID - DEST, DEST_CITY_NAME, DEST_CITY_MARKET_ID, DEST_STATE_ABR, DEST_STATE_NM, DEST_AIRPORT_ID, \n# 'DEP_TIME' - 'DEP_TIME_BLK', 'CRS_DEP_TIME',\n# ORIGIN - kept for primary join and will be dropped after\n\n# OUTPUT \n# 'DEP_DEL15' & 'CANCELLED'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b71fbe8-9e50-42d0-ba68-f8c4c177025b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_process_airlines(env='dev'):\n  \"\"\"\n  This function ingests raw airlines data and writes processed data to storage\n  input: env - dev/prod\n  output: None\n  \"\"\"\n  checkpoint = time.time()\n  if env == 'prod':\n  # Load 2015 Q1 for Flights\n    df_airlines = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/parquet_airlines_data/*\") \\\n                            .dropDuplicates() \\\n                            .cache()\n  \n  else: \n    df_airlines = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/parquet_airlines_data/*\") \\\n                            .filter((col('MONTH') == 1) & (col('ORIGIN').isin(['OAK', 'ATL', 'SJU']) )) \\\n                            .dropDuplicates() \\\n                            .cache()\n   \n#   checkpoint = time.time()\n    \n#   df_airlines.write.partitionBy('YEAR', 'ORIGIN') \\\n#                   .option(\"maxRecordsPerFile\", max_write_rows) \\\n#                   .mode(\"overwrite\") \\\n#                   .parquet(f\"{blob_url}/processed-{env}/df_airlines_deduped\")\n  \n#   df_airlines = spark.read.parquet(f\"{blob_url}/processed-{env}/df_airlines_deduped\").cache()  \n  \n#   stage_time = time.time() - checkpoint\n#   print(f\"{stage_time} sec - airlines data written and re-read\")\n#   checkpoint = time.time()\n  \n  \n  cancelled_cols = ['CANCELLED', 'CANCELLATION_CODE']\n\n  # ignored as they are known after and may cause data leakage\n  delay_cols = ['DEP_DELAY', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'FIRST_DEP_TIME', 'DIV_ARR_DELAY', 'DEP_DELAY_GROUP']\n  \n  airline_cols = ['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_CARRIER_AIRLINE_ID', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_STATE_FIPS', 'DEST_WAC', 'DEP_TIME', 'DEP_DEL15', 'DEP_DELAY_NEW', 'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP', 'DIV_AIRPORT_LANDINGS']\n  df_airlines = df_airlines.select(*airline_cols).cache()\n  \n  df_airlines.count()\n  \n  df_airlines.write.partitionBy('YEAR') \\\n                    .option(\"maxRecordsPerFile\", max_write_rows) \\\n                    .mode(\"overwrite\") \\\n                    .parquet(f\"{blob_url}/processed-{env}/df_airlines_deduped\")\n  \n  df_airlines = spark.read.parquet(f\"{blob_url}/processed-{env}/df_airlines_deduped\").cache()\n  \n  # time reported for departure and arrivals are in local time\n  # flights - FL_DATE YYYY-MM-DD, DEP_TIME 850, 1505\n  # df_airlines.select(col('DEP_TIME').cast('string').alias('DEP_TIME_OF')).show(5)\n  df_airlines = df_airlines.withColumn('DEP_TIME', df_airlines['DEP_TIME'])\n  timestamp_dep_time = to_timestamp('DEP_TIME')\n  df_airlines = df_airlines.withColumn( 'DEP_TIME_OF', timestamp_dep_time)\n\n  # actual conversion of time col to time type\n  # df_airlines = df_airlines.withColumn(\"DEP_HOUR_OF\", split(col(\"DEP_TIME\").cast(StringType()), \"..$\").getItem(0).cast(IntegerType()))\n  \n  # format and bin time for intervals\n  dep_time_hour = split(col(\"DEP_TIME\").cast(StringType()), \"..$\").getItem(0).cast(IntegerType())\n\n  # convert categoricals - https://spark.apache.org/docs/latest/ml-features.html#stringindexer\n  indexer = StringIndexer(inputCol='TAIL_NUM', outputCol=\"TAIL_NUM_NEW\")\n  df_airlines_post = indexer.fit(df_airlines).transform(df_airlines)\n\n  # extract hour from departure\n  dateformat_dep_time = date_format(col('DEP_TIME_OF'), 'HH:mm')\n  df_airlines_post = df_airlines.withColumn('DEP_TIME_OF', dateformat_dep_time)\n  df_airlines_post = df_airlines.withColumn('DEP_HOUR_OF', dep_time_hour).drop('DEP_TIME_OF').cache()\n  \n  # convert local timezones to UTC - weather data is reported in UTC\n  airports = airportsdata.load('IATA')\n  timezones_dict = {airport_code: airports[airport_code][\"tz\"] for airport_code in airports}\n  mapping_expr = create_map([lit(x) for x in chain(*timezones_dict.items())])\n  df_airlines_post = df_airlines_post.withColumn(\"TIMEZONE\", mapping_expr[col(\"ORIGIN\")])\n\n  ## Create a datetime column (to accomodate for day changes)\n  df_airlines_post = df_airlines_post.withColumn(\"DEP_HOUR_OF_FORMATTED\", concat_ws(\":\", col(\"DEP_HOUR_OF\"), lit(\"00\"), lit(\"00\")))\n  df_airlines_post = df_airlines_post.withColumn(\"FL_DATETIME\", concat_ws(\" \", col(\"FL_DATE\"), col(\"DEP_HOUR_OF_FORMATTED\")))\n\n  ## Create a new UTC version of the hour column\n  df_airlines_post = df_airlines_post.withColumn(\"FL_DATETIME_UTC\", to_utc_timestamp(to_timestamp(col(\"FL_DATETIME\"), \"yyyy-MM-dd H:mm:ss\"), col(\"TIMEZONE\")))\n\n  ## Truncate to just the UTC hour\n  df_airlines_post = df_airlines_post.withColumn(\"DEP_HOUR_OF_UTC\", hour(col(\"FL_DATETIME_UTC\")))\n  \n  stage_time = time.time() - checkpoint\n  print(f\"{stage_time} sec - airlines data processed\")\n  checkpoint = time.time()\n  df_airlines_post.count()\n  \n  # write the processed airlines data to disk\n  df_airlines_post.write.partitionBy('YEAR', 'ORIGIN') \\\n                        .option(\"maxRecordsPerFile\", max_write_rows) \\\n                        .mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_airlines_post\")\n  \n  stage_time = time.time() - checkpoint\n  print(f\"{stage_time} sec - airlines data written\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"360390e7-a859-4cb5-8dc5-b7d2731da900"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_airlines(env='dev'):\n  \"\"\"\n  This function reads processed airlines data from storage\n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  df_airlines_post = spark.read.parquet(f\"{blob_url}/processed-{env}/df_airlines_post\").cache()\n  return df_airlines_post"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1922fc80-307f-4710-9c84-f2d9e359c4c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_airlines(env='dev', write=False):\n  \"\"\"\n  This function allows data pipeline to be rerun and to overwrite what is in storage\n  then reads what is in storage \n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_process_airlines(env=STAGE)\n  else:\n    pass\n  df_airlines_post = read_airlines(env=STAGE).cache()\n  print(f\"Your new df_airlines has {df_airlines_post.count():,} rows.\")\n  return df_airlines_post"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ce9cc5a-bf58-421b-b019-67d142236bc5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_airports_list(df_airlines_post, env='dev'):\n  \"\"\"\n  This uses the airlines processed data and gets a list of the airports\n  input: dataframe\n  output: list\n  \"\"\"\n  # get list of airports (neighbor_call) and weather stations (neighbor_id) in the data \n  airports_list = df_airlines_post.select(\"ORIGIN\").distinct().rdd.flatMap(lambda x: x).collect()\n  # the neighbor_id's in stations all start with k. \n  airports_list = [\"K\" + x for x in airports_list]\n  return airports_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cb480c1-eacf-46e3-9343-a0c7bfadcf05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Stations Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1c12851-7d22-4cff-84ce-f4abbe00cd18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_process_stations(airports_list, env='dev'):\n  \"\"\"\n  This function ingests raw stations data and writes processed data to storage\n  input: env - dev/prod\n  output: None\n  \"\"\"\n  if env == 'prod':\n  # Load 2015 Q1 for Flights\n    df_stations = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/stations_data/*\").filter(col('NEIGHBOR_CALL').isin(airports_list)).cache()\n  \n  else: \n    df_stations = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/stations_data/*\").filter(col('NEIGHBOR_CALL').isin(airports_list)).cache()\n    \n  # neighbor call signs have additional k at start and should be removed. \n  call_sign_expr = \"^.{1}\"\n  regex_replace_call_sign = regexp_replace('neighbor_call', call_sign_expr, '')\n\n  # drop duplicative location related columns\n  df_stations = df_stations.drop('usaf', 'wban')\n  \n  df_stations_post = df_stations.withColumn('neighbor_call',  regex_replace_call_sign).drop('neighbor_name', 'neighbor_state').cache()\n  \n  df_stations_post.count()\n  # write the processed airlines data to disk\n  df_stations_post.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_stations_post\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcd6bab1-b7f7-4cc5-8c8e-77e498d8b02c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_stations(env='dev'):\n  \"\"\"\n  This function reads processed airlines data from storage\n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  df_stations_post = spark.read.parquet(f\"{blob_url}/processed-{env}/df_stations_post\").cache()\n  return df_stations_post"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"391a8259-dcb4-438c-95e1-2659d080b0b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_stations(airports_list, env='dev', write=False):\n  \"\"\"\n  This function allows data pipeline to be rerun and to overwrite what is in storage\n  then reads what is in storage \n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_process_stations(airports_list, env=STAGE)\n  else:\n    pass\n  df_stations_post = read_stations(env=STAGE).cache()\n  print(f\"Your new df_stations has {df_stations_post.count():,} rows.\")\n  return df_stations_post"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e5a4f2-f87b-464c-b476-af71c23b712d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_agg_stations(df_stations_post, env='dev'):\n  \"\"\"\n  There are multiple stations for each airport. keeping all will make our data much bigger with multiple stations per flight\n  This function the weather station with minimum distance to each airport \n  then writes processed data to storage\n  input: \n  df_stations_post - processd stations data\n  env - dev/prod\n  output: None\n  \"\"\"\n  windowNeighborDist = Window.partitionBy(\"neighbor_call\").orderBy(col(\"distance_to_neighbor\").asc())\n  df_stations_agg = df_stations_post.withColumn(\"row\",row_number() \\\n                               .over(windowNeighborDist)) \\\n                               .filter(col(\"row\") == 1).drop(\"row\") \\\n                               .cache()\n  \n  df_stations_agg.count()\n  df_stations_agg.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_stations_agg\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5deff617-5673-4d08-8214-f1c152431a02"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_stations_agg(env='dev'):\n  \"\"\"\n  This function reads processed stations data from storage\n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  df_stations_agg = spark.read.parquet(f\"{blob_url}/processed-{env}/df_stations_agg\").cache()\n  return df_stations_agg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"846f3168-0e3a-4f2d-b6af-dc83d70edc48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_stations_agg(df_stations_post, env='dev', write=False):\n  \"\"\"\n  This function allows data pipeline to be rerun and to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_stations_post - processed dataframe of stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_agg_stations(df_stations_post, env=STAGE)\n  else:\n    pass\n  df_stations_agg = read_stations_agg(env=STAGE).cache()\n  print(f\"Your new df_stations_agg has {df_stations_agg.count():,} rows.\")\n  return df_stations_agg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93405105-bf79-48b5-864d-4fe2f03cd943"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_joined_flights(df_airlines_post, df_stations_agg, env='dev'):\n  \"\"\"\n  This function joins airline and staion data and overwrites what is in storage\n  input: \n  df_airlines_post - processed airlines data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: None\n  \"\"\"\n  # Join stations data with flights data\n  df_flights = df_airlines_post.join(df_stations_agg, df_airlines_post[\"ORIGIN\"] ==  df_stations_agg[\"NEIGHBOR_CALL\"], \"left\").cache()\n  df_flights.count()\n  df_flights.write.mode(\"overwrite\").partitionBy('YEAR') \\\n                                    .option(\"maxRecordsPerFile\", max_write_rows) \\\n                                    .parquet(f\"{blob_url}/processed-{env}/df_flights\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbbd57c-c457-428c-884e-c37f2060cc62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_flights(env='dev'):\n  \"\"\"\n  This function reads joined airline and station data from storage\n  input: env - dev/prod\n  output: dataframe\n  \"\"\"\n  df_flights = spark.read.parquet(f\"{blob_url}/processed-{env}/df_flights\").cache()\n  return df_flights"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e90963df-4384-40e9-9436-07854c0dda6a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_flights(df_airlines_post, df_stations_agg, env='dev', write=False):\n  \"\"\"\n  This function allows airline and station data join to be rerun and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_airlines_post - processed airlines data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_joined_flights(df_airlines_post, df_stations_agg, env=STAGE)\n  else:\n    pass\n  df_flights = read_flights(env=STAGE).cache()\n  print(f\"Your new df_flights has {df_flights.count():,} rows.\")\n  return df_flights"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16edfe6e-1fe4-4e78-adbc-f9f030c9163a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_weather_filters(df_flights):\n  \"\"\"\n  this takes flights data and gets the distinct stations, months, and years of the data\n  input: dataframe\n  output: list of lists\n  \"\"\"\n  # get list of weather stations (neighbor_id) in the data \n  stations_list = df_flights.select(\"NEIGHBOR_ID\").distinct().rdd.flatMap(lambda x: x).collect()\n  try:\n    stations_list.remove(None)\n  except:\n    None\n  flightYears_list = df_flights.select(\"YEAR\").distinct().rdd.flatMap(lambda x: x).collect()\n  flightMonths_list = df_flights.select(\"MONTH\").distinct().rdd.flatMap(lambda x: x).collect()\n  return [stations_list, flightMonths_list, flightYears_list]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"913dd886-2fcc-4d1a-9c35-d5bb1325156f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Weather Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e63cb91a-f510-46da-bb1e-91d579769dc8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_process_weather(stations_list, env='dev'):\n  \"\"\"\n  This function ingests raw weather data and writes processed data to storage\n  input: \n  list of stations\n  output: None\n  \"\"\"\n  \n  # In some cases, no observations are made and they are recorded across all weather columns as a full set of 9's\n  no_observations = \"col('WND') != '999,9,9,9999,9'\"\n  \n  checkpoint = time.time()\n  if env == 'prod':\n  # Load 2015 Q1 for Flights\n    df_weather = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/weather_data/*\").filter( (col('station').isin(stations_list)) & eval(no_observations) ).cache()\n  \n  else: \n    df_weather = spark.read.parquet(\"/mnt/mids-w261/datasets_final_project/weather_data/*\").filter( (col('station').isin(stations_list)) & eval(no_observations) ).cache()\n  \n#   stage_time = time.time() - checkpoint\n#   print(f\"{stage_time} sec - weather data read\")\n#   checkpoint = time.time()\n    \n#   df_weather.write.partitionBy('STATION', 'CALL_SIGN') \\\n#                   .option(\"maxRecordsPerFile\", max_write_rows) \\\n#                   .mode(\"overwrite\") \\\n#                   .parquet(f\"{blob_url}/processed-{env}/df_weather_filtered\")\n  \n#   df_weather = spark.read.parquet(f\"{blob_url}/processed-{env}/df_weather_filtered\").cache()  \n  \n#   stage_time = time.time() - checkpoint\n#   print(f\"{stage_time} sec - weather data written and re-read\")\n#   checkpoint = time.time()\n    \n  # https://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/\n  # weather data dictionary - https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf\n  split_wnd = split(df_weather['WND'], ',') # Wind direction and speed, 999. if type_code=v, 999 is variable\n  split_cig = split(df_weather['CIG'], ',') \n  split_vis = split(df_weather['VIS'], ',') # Visibility\n  split_tmp = split(df_weather['TMP'], ',') # Air temperature\n  split_dew = split(df_weather['DEW'], ',') # Dewpoint\n  split_slp = split(df_weather['SLP'], ',') # Sea level atmospheric pressure\n  split_date_t = split(df_weather['DATE'], 'T')  # splits date from time, 'DATE' 2015-01-01T00:16:00.000+0000\n  trim_station = trim(df_weather['STATION'])\n\n  df_weather = df_weather.withColumn('STATION', trim_station) \\\n                         .withColumn('DATE_SPLIT', split_date_t.getItem(0)) \\\n                         .withColumn('WND_ANGLE', split_wnd.getItem(0).cast('integer')) \\\n                         .withColumn('WND_DIR', split_wnd.getItem(2)) \\\n                         .withColumn('WND_SPEED', split_wnd.getItem(3).cast('integer')) \\\n                         .withColumn('CIG_DIM', split_cig.getItem(0).cast('integer')) \\\n                         .withColumn('VIS_DIST', split_vis.getItem(0).cast('integer')) \\\n                         .withColumn('TMP_AIR', split_tmp.getItem(0)) \\\n                         .withColumn('DEW_POINT', split_dew.getItem(0)) \\\n                         .withColumn('SLP_DAY', split_slp.getItem(0).cast('integer')).cache()\n\n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather values split\")\n  checkpoint = time.time()\n  \n  plus_expr = \"^.{1}\" # temps have \"+\" character at start\n  regex_replace_air = regexp_replace('TMP_AIR', plus_expr, '')\n  regex_replace_dew = regexp_replace('DEW_POINT', plus_expr, '')\n  split_date_space = split(df_weather['DATE_SPLIT'], ' ') # split of date and time\n  df_weather = df_weather.withColumn('TMP_AIR',  regex_replace_air.cast('integer')) \\\n                         .withColumn('DEW_POINT',  regex_replace_dew.cast('integer')) \\\n                         .withColumn('DATE_OBS', split_date_space[0]) \\\n                         .withColumn('TIME_OBS', split_date_space[1]).cache()\n\n  # replace all missing values with an empty value. dont want them to skew an average\n  df_weather = df_weather.withColumn('WND_ANGLE', regexp_replace('WND_ANGLE', '999', '')) \\\n                         .withColumn('WND_SPEED', regexp_replace('WND_SPEED', '999', '')) \\\n                         .withColumn('CIG_DIM', regexp_replace('CIG_DIM', '999', '')) \\\n                         .withColumn('VIS_DIST', regexp_replace('VIS_DIST', '9999', '')) \\\n                         .withColumn('TMP_AIR', regexp_replace('TMP_AIR', '9999', '')) \\\n                         .withColumn('DEW_POINT', regexp_replace('DEW_POINT', '9999', '')) \\\n                         .withColumn('SLP_DAY', regexp_replace('SLP_DAY', '99999', '')).cache()\n\n\n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather missing 999 values replaced\")\n  checkpoint = time.time()\n  \n  # replace empty with null values\n  df_weather = df_weather.withColumn(\"AW1\", when(col(\"AW1\")==\"\" ,None).otherwise(col(\"AW1\"))) \\\n                   .withColumn(\"WND_ANGLE\", when(col(\"WND_ANGLE\")==\"\" ,None).otherwise(col(\"WND_ANGLE\"))) \\\n                   .withColumn(\"WND_SPEED\", when(col(\"WND_SPEED\")==\"\" ,None).otherwise(col(\"WND_SPEED\"))) \\\n                   .withColumn(\"CIG_DIM\", when(col(\"CIG_DIM\")==\"\" ,None).otherwise(col(\"CIG_DIM\"))) \\\n                   .withColumn(\"VIS_DIST\", when(col(\"VIS_DIST\")==\"\" ,None).otherwise(col(\"VIS_DIST\"))) \\\n                   .withColumn(\"TMP_AIR\", when(col(\"TMP_AIR\")==\"\" ,None).otherwise(col(\"TMP_AIR\"))) \\\n                   .withColumn(\"DEW_POINT\", when(col(\"DEW_POINT\")==\"\" ,None).otherwise(col(\"DEW_POINT\"))) \\\n                   .withColumn(\"SLP_DAY\", when(col(\"SLP_DAY\")==\"\" ,None).otherwise(col(\"SLP_DAY\"))).cache()\n  \n  \n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather columns replaced with null values\")\n  checkpoint = time.time()\n  \n  dateformat_time_obs = date_format('TIME_OBS', 'HH:mm') # Format time to match airline time format\n  df_weather = df_weather.withColumn('TIME_OBS', dateformat_time_obs) \\\n                         .withColumn('HOUR_OBS', hour(col('TIME_OBS'))) \\\n                         .withColumn('YEAR_OBS', year(col('DATE_OBS'))) \\\n                         .withColumn('MONTH_OBS', month(col('DATE_OBS'))).cache()\n\n  \n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather hour, month, and year extracted\")\n  checkpoint = time.time()\n  \n  \n  # forward fill data - we know that data is only being reported when there is a change\n  cols = ['DATE_SPLIT', 'WND_ANGLE', 'WND_SPEED', 'CIG_DIM', 'VIS_DIST', 'WND_DIR', 'TMP_AIR', 'DEW_POINT', 'SLP_DAY', 'AW1']\n  w1 = Window.orderBy('DATE_SPLIT')\n  w2 = w1.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n  df_weather = df_weather.select([ c for c in df_weather.columns if c not in cols ] + [ coalesce(last(c,True).over(w1), first(c,True).over(w2)).alias(c) for c in cols ]).cache()\n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather data filled forward\")\n  checkpoint = time.time()\n  \n  # time is being reported in UTC\n  # want time to be reporting for 2 hours before the flight time\n  weather_obs_time = 120 # number of minutes for observation before the flight\n  df_weather = df_weather.withColumn('FLIGHT_TIME_OBS', df_weather['TIME_OBS'] + expr(f'INTERVAL {weather_obs_time} MINUTES'))\n  dateformat_dep_time = date_format('FLIGHT_TIME_OBS', 'HH:mm')\n  \n  df_weather_post = df_weather.withColumn( 'FLIGHT_TIME_OBS', dateformat_dep_time) \\\n                         .withColumn('FLIGHT_HOUR_OBS', hour(col('TIME_OBS'))) \\\n                         .drop('NAME', 'REPORT_TYPE', 'QUALITY_CONTROL', 'REM', 'EQD').cache()\n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather date columns adjusted for flight times\")\n  checkpoint = time.time()\n  \n  df_weather_post.count()\n  df_weather_post.write.partitionBy('YEAR_OBS') \\\n                       .option(\"maxRecordsPerFile\", max_write_rows) \\\n                       .mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_weather_post\")\n  \n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - weather data write to storage complete\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7b28583-97ec-4f90-a4fb-c38e9bf27c3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_weather(env='dev'):\n  df_weather = spark.read.parquet(f\"{blob_url}/processed-{env}/df_weather_post\").cache()\n  return df_weather"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d9f12dd-fb72-43bc-880a-1cad25a21864"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_filter_weather(flightMonths_list, flightYears_list, df_weather_post, env='dev'):\n  # for departure times, remove null value rows - is really only relevant to dev data processing where we choose specific months\n  df_weather_post = df_weather_post.filter((col('MONTH_OBS').isin(flightMonths_list)) & (col('YEAR_OBS').isin(flightYears_list))).cache()\n  df_weather_post.count()\n  df_weather_post.write.option(\"maxRecordsPerFile\", max_write_rows) \\\n                       .mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_weather_post_filtered\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a28ece4-85f9-4396-a025-47d521947163"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_weather_filtered(env='dev'):\n  df_weather = spark.read.parquet(f\"{blob_url}/processed-{env}/df_weather_post_filtered\").cache()\n  return df_weather"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac4db5f8-42ba-4c16-8a5a-160f5b78a0cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_weather(stations_list, flightMonths_list, flightYears_list, env='dev', write=False):\n  \"\"\"\n  This function allows weather data processing and filtering to be rerun and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_airlines_post - processed airlines data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_process_weather(stations_list, env=STAGE)\n    df_weather = read_weather(env=STAGE).cache()\n    write_filter_weather(flightMonths_list, flightYears_list, df_weather, env=STAGE)\n  else:\n    pass\n  df_weather_post = read_weather_filtered(env=STAGE).cache()\n  print(f\"Your new df_weather has {df_weather_post.count():,} rows.\")\n  return df_weather_post"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c67f48e4-a2fc-47e6-88ab-19eea2b0b9b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_weather_agg(df_weather_post, env='dev'):\n  # Want a single aggregated weather observation for each time period \n  df_weather_agg = df_weather_post.groupBy('STATION', 'DATE_OBS', 'MONTH_OBS', 'YEAR_OBS', 'FLIGHT_HOUR_OBS') \\\n                             .agg(avg('WND_ANGLE').alias('WND_ANGLE'), \\\n                                  avg('WND_SPEED').alias('WND_SPEED'), \\\n                                  avg('CIG_DIM').alias('CIG_DIM'), \\\n                                  avg('VIS_DIST').alias('VIS_DIST'), \\\n                                  avg('TMP_AIR').alias('TMP_AIR'), \\\n                                  avg('DEW_POINT').alias('DEW_POINT'), \\\n                                  avg('SLP_DAY').alias('SLP_DAY')\n                                 ).cache()\n  df_weather_post.unpersist()\n  df_weather_agg.count()\n  df_weather_agg.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", max_write_rows) \\\n                                        .parquet(f\"{blob_url}/processed-{env}/df_weather_agg\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ffe20d3-c9ad-4ad9-9e61-cb256553ae19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_weather_agg(env='dev'):\n  df_weather_agg = spark.read.parquet(f\"{blob_url}/processed-{env}/df_weather_agg\").cache()\n  return df_weather_agg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56d88f53-751d-4217-aad1-41567969eb61"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_weather_agg(df_weather_post, env='dev', write=False):\n  \"\"\"\n  This function allows weather data to be aggregated and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_weather_post - processed weather data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_weather_agg(df_weather_post, env=STAGE)\n  else:\n    pass\n  df_weather_agg = read_weather_agg(env=STAGE).cache()\n  print(f\"Your new df_weather_agg has {df_weather_agg.count():,} rows.\")\n  return df_weather_agg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4307fb7-c958-4241-9c58-d7edf75423e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_joined_weather_stations(df_weather_agg, df_stations_agg, env='dev'):\n  \"\"\"\n  This function joins airline and staion data and overwrites what is in storage\n  input: \n  df_airlines_post - processed airlines data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: None\n  \"\"\"\n  # join weather data with flights data. drop station columns that will be considered duplicate in final join\n  stations_drop_list = ['NEIGHBOR_ID','distance_to_neighbor', 'lat', 'lon', 'neighbor_call', 'neighbor_lat', 'neighbor_lon', 'neighbor_name', 'neighbor_state', 'station_id', 'usaf', 'wban']\n  df_weather_stations = df_weather_agg.join(df_stations_agg, df_weather_agg[\"STATION\"] ==  df_stations_agg[\"NEIGHBOR_ID\"], \"inner\").drop(*stations_drop_list)\n  df_weather_stations.count()\n  df_weather_stations.write.partitionBy('YEAR_OBS') \\\n                          .option(\"maxRecordsPerFile\", max_write_rows) \\\n                          .mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_weather_stations\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cb98d9a-1b1f-4759-a304-9e505bc4a7c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_joined_weather_stations(env='dev'):\n  df_weather_stations = spark.read.parquet(f\"{blob_url}/processed-{env}/df_weather_stations\").cache()\n  return df_weather_stations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6ef5945-d445-4503-9417-4b515c8753b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_joined_weather_stations(df_weather_agg, df_stations_agg, env='dev', write=False):\n  \"\"\"\n  This function allows join of weather data and stations data to be joined and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_weather_agg - aggregated weather data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_joined_weather_stations(df_weather_agg, df_stations_agg, env=STAGE)\n  else:\n    pass\n  df_weather_stations = read_joined_weather_stations(env=STAGE).cache()\n  df_weather_stations.count()\n  return df_weather_stations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94d3e3a8-d64f-4c2d-8ffe-7e211ebb5b41"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_joined_flights_weather(df_flights, df_weather_stations, env='dev'):\n  \"\"\"\n  This function joins flights and weather data and overwrites what is in storage\n  input: \n  df_flights - processed airlines data\n  df_weather_stations - aggregated stations data\n  env - dev/prod\n  output: None\n  \"\"\"\n  # we want to combine on location (station) and datetime (hour)\n  # flights - FL_DATE YYYY-MM-DD, DEP_HOUR_OF_UTC \n  # weather - DATE_OBS FLIGHT_HOUR_OBS\n  # we will also drop string columns only used for the join\n  drop_cols = ['FL_DATE', 'ORIGIN', 'DATE_OBS', 'TAIL_NUM', 'STATION_ID', 'STATION', 'neighbor_id','neighbor_call', 'DEP_HOUR_OF_FORMATTED',  'FL_DATETIME', 'FL_DATETIME_UTC', 'TIMEZONE']\n  df_flights_weather = df_flights.join(df_weather_stations, (df_flights[\"NEIGHBOR_ID\"] ==  df_weather_stations[\"STATION\"]) & (df_flights[\"FL_DATE\"] == df_weather_stations[\"DATE_OBS\"]) & (df_flights[\"DEP_HOUR_OF_UTC\"] ==  df_weather_stations[\"FLIGHT_HOUR_OBS\"]), \"left\").drop(*drop_cols).cache()\n  df_flights_weather = df_flights_weather.withColumn(\"id\", monotonically_increasing_id())\n  df_flights_weather.count()\n  df_flights_weather.write.option(\"maxRecordsPerFile\", max_write_rows) \\\n                          .mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_flights_weather\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a79547b8-1faf-489c-ac50-79cc959f415d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_joined_flights_weather(env='dev'):\n  df_flights_weather = spark.read.parquet(f\"{blob_url}/processed-{env}/df_flights_weather\").cache()\n  return df_flights_weather"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dd5e3fd-599a-4ce8-b5e3-86273e8efb38"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_joined_flights_weather(df_flights, df_weather_stations, env='dev', write=False):\n  \"\"\"\n  This function allows join of weather data and stations data to be joined and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_weather_agg - aggregated weather data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  STAGE = env\n  if write == True:\n    write_joined_flights_weather(df_flights, df_weather_stations, env=STAGE)\n  else:\n    pass\n  df_flights_weather = read_joined_flights_weather(env=STAGE).cache()\n  print(f\"Your new df_flights_weather has {df_flights_weather.count():,} rows.\")\n  return df_flights_weather"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d92c3b6c-6de9-4d30-9b42-c8aa8dd6413a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Process Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10ea8da4-5eea-40f1-a814-792602709fb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def process_pipeline(env='dev', write=False):\n  \"\"\"\n  This function allows the full pipeline to be run\n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  env - dev/prod\n  write - True/False\n  output: dataframe\n  \"\"\"\n\n  write_flag = write\n  STAGE = env\n  if write == True:\n    checkpoint = time.time()\n    airlines_data = get_airlines(env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - airlines processed\")\n    checkpoint = time.time()\n    a_list = get_airports_list(airlines_data, env=STAGE)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - retrieved distinct airline list\")\n    checkpoint = time.time()\n    stations_data = get_stations(a_list, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - stations for select airlines processed\")\n    checkpoint = time.time()\n    agg_stations_data = get_stations_agg(stations_data, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - stations data minimized\")\n    checkpoint = time.time()\n    flights_data = get_flights(airlines_data, agg_stations_data, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - airlines and station data joined\")\n    airlines_data.unpersist()\n    stations_data.unpersist() \n    checkpoint = time.time()\n    w_stations, w_months, w_years = get_weather_filters(flights_data)\n    weather_data = get_weather(w_stations, w_months, w_years, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - weather data processed\")\n    checkpoint = time.time()\n    agg_weather_data = get_weather_agg(weather_data, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - weather data aggregated\")\n    weather_data.unpersist()\n    checkpoint = time.time()\n    weather_stations_data = get_joined_weather_stations(agg_weather_data, agg_stations_data, env=STAGE, write=write_flag)\n    stage_time = time.time() - checkpoint\n    # print(f\"{stage_time} sec - weather and stations data joined\")\n    agg_weather_data.unpersist()\n    agg_stations_data.unpersist()\n  else: \n    flights_data = None\n    weather_stations_data = None\n  checkpoint = time.time()\n  flights_weather_data = get_joined_flights_weather(flights_data, weather_stations_data, env=STAGE, write=write_flag)\n  stage_time = time.time() - checkpoint\n  # print(f\"{stage_time} sec - join time of weather and flights data\")\n  if write:\n    flights_data.unpersist()\n    weather_stations_data.unpersist()\n  flights_weather_data.count()\n  return flights_weather_data\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351887c3-e34d-4798-8c9d-5d222e5a090c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#### Data Transform and Split"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e593ce7-2533-414b-9a66-acf4f7193af3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_feature_cols(dataframe, env='dev'):\n  label_cols = ['DEP_DEL15', 'DEP_DELAY_NEW']\n  feature_cols = dataframe.drop(*label_cols).columns\n  return feature_cols"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b44cf9c-3d8e-47af-9c80-9a28764432f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_vectorassembed(dataframe, env='dev'):\n  \"\"\"\n  This function will translate a given dataframe into one with a features and an output column\n  \"\"\"\n  STAGE=env\n  feature_cols = get_feature_cols(dataframe, env=STAGE)\n\n  assembler = VectorAssembler(\n      inputCols=feature_cols,\n      outputCol=\"features\")\n  \n  dataframe_vec_cols = assembler.setParams(handleInvalid=\"skip\").transform(dataframe).cache()\n  \n  return dataframe_vec_cols"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fbe5e89-aeb8-4fb3-9be0-fd0f8bf3e714"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def poormans_split(env='dev'):\n  \"\"\"\n  This function first handles vector assembly with feature columns and then \n  does a silly split by years for a larger dataset into 3 groups\n  input: single dataframe\n  output: 3 dataframes\n  \"\"\"\n  # logistic reg. takes one column value as input \n  # vector assembly to translate columns into single vector column \n  # transform columns into single column feature dataset\n  # we skip invalids as logistic reg. will not take NaN\n  # https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html#pyspark.ml.feature.VectorAssembler.handleInvalid\n  # get columns \n  STAGE = env\n  df_flights_weather = read_joined_flights_weather(env=STAGE).cache()\n  \n  flights_weather_vec_cols = get_vectorassembed(df_flights_weather, env=STAGE)\n  df_flights_weather.unpersist()\n  \n  train_log = flights_weather_vec_cols.filter(col('YEAR').isin([2015,2016])).cache()\n  validation_log = flights_weather_vec_cols.filter(col('YEAR').isin([2017,2018])).cache()\n  test_log = flights_weather_vec_cols.filter(col('YEAR') == 2019).cache()\n  \n  train_count = train_log.count()\n  validation_count = validation_log.count()\n  test_count = test_log.count()\n  flights_weather_vec_cols.unpersist()\n  print(f\"Your testing datasets have {train_count:,}, {validation_count:,}, and {test_count:,} rows.\")\n  return train_log, validation_log, test_log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deb77791-4d0c-4e4a-9787-a3a407d6d852"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def model_performance_charts(log_Model):\n  beta = np.sort(log_Model.coefficients)\n  plt.plot(beta)\n  plt.ylabel('Beta Coefficients')\n  plt.show()\n  trainingSummary = log_Model.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n  pr = trainingSummary.pr.toPandas()\n  plt.plot(pr['recall'],pr['precision'])\n  plt.ylabel('Precision')\n  plt.xlabel('Recall')\n  plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e42fe5b3-1b0e-4f74-8e62-fdb06941847a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def model_perf_summary(log_Model):\n  # model evaluation https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\n  trainingSummary = log_Model.summary\n  accuracy = trainingSummary.accuracy\n  falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n  truePositiveRate = trainingSummary.weightedTruePositiveRate\n  fMeasure = trainingSummary.weightedFMeasure()\n  precision = trainingSummary.weightedPrecision\n  recall = trainingSummary.weightedRecall\n  areaUnderROC = trainingSummary.areaUnderROC\n  print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54f8c5ab-4b4c-43f8-81f2-fece7860bd2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_split_data(dataframe, env='dev', features='dev'):\n  \"\"\"\n  This defines the splits of the data by year\n  \"\"\"\n  if features == 'prod':\n    model = \"pca\"\n  else:\n    feature_col = 'features'\n    model = features\n  # hold-out test set\n  hold_out_variable = 'YEAR'\n  test_years = [2019]\n\n  years_list = dataframe.select(hold_out_variable).distinct().rdd.flatMap(lambda x: x).collect()\n  train_years = list(set(years_list) - set([2019]))\n\n  # train and test set\n#   trainDF = read_joined_flights_weather(env=STAGE).filter(col(hold_out_variable).isin(train_years) ).cache()\n#   testDF = read_joined_flights_weather(env=STAGE).filter(col(hold_out_variable).isin(test_years) ).cache()\n  trainDF = dataframe.filter(col(hold_out_variable).isin(train_years)).cache()\n  testDF = dataframe.filter(col(hold_out_variable).isin(test_years) ).cache()\n  \n  # time to beat - 1.07 min\n  trainDF.count()\n  trainDF.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/trainDF-{model}_features\")\n  testDF.count()\n  testDF.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/testDF-{model}_features\")\n  \n  trainDF = spark.read.parquet(f\"{blob_url}/processed-{env}/trainDF-{model}_features\").cache()\n  testDF = spark.read.parquet(f\"{blob_url}/processed-{env}/testDF-{model}_features\").cache()\n  \n  # time to beat - 38 sec\n  trainDF.groupBy('YEAR').count()\n  testDF.groupBy('YEAR').count()\n  \n  # trainDF.groupBy('DEP_DEL15').count().show()\n  \n  # split the data given labels\n  minor_df = trainDF.filter(col('DEP_DEL15')==1).cache()\n  major_df = trainDF.filter(col('DEP_DEL15')==0).cache()\n  trainDF.unpersist()\n  trainDF.unpersist()\n  \n  minor_df.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/minor_df-{model}_features\")\n  major_df.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/major_df-{model}_features\")\n  \n  minor_df = spark.read.parquet(f\"{blob_url}/processed-{env}/minor_df-{model}_features\").cache()\n  major_df = spark.read.parquet(f\"{blob_url}/processed-{env}/major_df-{model}_features\").cache()\n\n  n_ontime = major_df.count()\n  n_delays = minor_df.count()\n  \n  ratio = n_ontime/n_delays\n  print('The ratio of on-time to delayed flights is of {:0.1f}:1'.format(ratio))\n\n  \n  # time to beat - 2.85 min\n  \n  # oversample the delayed flights\n  oversample_df = minor_df.sample(withReplacement=True, fraction=ratio, seed=321)\n  df_augmentedTrain = major_df.unionAll(oversample_df).cache()\n  \n  \n  # df_augmentedTrain.groupBy('DEP_DEL15').count().show()\n  df_augmentedTrain.count()\n  major_df.unpersist()\n  minor_df.unpersist()\n  \n  df_augmentedTrain.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{env}/df_augmentedTrain-{model}_features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"626a9bd5-153a-42ff-b9e8-14bf92e0216d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_split_data(env='dev', features='dev'):\n  if features == 'prod':\n    model = \"pca\"\n  else:\n    feature_col = 'features'\n    model = features\n  trainDF = spark.read.parquet(f\"{blob_url}/processed-{env}/df_augmentedTrain-{model}_features\").cache()\n  testDF = spark.read.parquet(f\"{blob_url}/processed-{env}/testDF-{model}_features\").cache()\n  return trainDF, testDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daed15e9-717f-4d66-8811-1c03e494d2eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_split_data(dataframe, env='dev', features='dev', write=False):\n  \"\"\"\n  This function allows join of weather data and stations data to be joined and \n  to overwrite what is in storage\n  then reads what is in storage \n  input: \n  df_weather_agg - aggregated weather data\n  df_stations_agg - aggregated stations data\n  env - dev/prod\n  output: dataframe\n  \"\"\"\n  FEATURES = features\n  STAGE = env\n  if write == True:\n    write_split_data(dataframe, env=STAGE, features=FEATURES)\n  else:\n    pass\n  df_train, df_test = read_split_data(env=STAGE, features=FEATURES)\n  print(f\"Your new df_train has {df_train.count():,} rows.\")\n  print(f\"Your new df_test has {df_test.count():,} rows.\")\n  return df_train, df_test"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f990b3a-b2ce-4a43-b496-be784b9b17dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def count_folds(dataframe):\n  \"\"\"\n  returns the total number of folds in the dataset\n  \"\"\"\n  dataframe_vec = get_vectorassembed(dataframe, env=STAGE).cache()\n  fold_variable = 'YEAR'\n  fold_list = dataframe_vec.select(fold_variable).distinct().toPandas()[fold_variable]\n  mapping = {x: x - fold_list.min() for x in fold_list}\n  print(f'{fold_variable.capitalize()}, fold_number mapping: {mapping}')\n\n  # define number of Folds as nYEARS - 1\n  nFolds = len(fold_list) - 1\n  return nFolds"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3ce56fd-7afe-44fd-bec3-09c96cb2e84e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def write_cvdata(train_data, test_data, env='dev', features='dev'):\n  \"\"\"\n  This functions determines the number of folds for each year and creates a dataset with a new column showing which set it is a part of\n  \"\"\"\n  \n  STAGE = env\n  # extract number of distinct YEARS in the training data and create a map\n  fold_variable = 'YEAR'\n  \n  if features == 'prod':\n    feature_col = 'pcaFeatures'\n    train_df_vec = train_data\n    test_df_vec = test_data\n    model = \"pca\"\n  else:\n    feature_col = 'features'\n    # perform vector assembly\n    # transform split data to an assembly vector - perform after the split as it might write new rows to oversample\n    train_df_vec = get_vectorassembed(train_data, env=STAGE).cache()\n    test_df_vec = get_vectorassembed(test_data, env=STAGE).cache()\n    # train_df_vec = train_df.cache()\n    # test_df_vec = test_df.cache()\n    model = features\n  \n  fold_list = train_df_vec.select(fold_variable).distinct().toPandas()[fold_variable]\n  mapping = {x: x - fold_list.min() for x in fold_list}\n  print(f'{fold_variable.capitalize()}, fold_number mapping: {mapping}')\n\n  # define number of Folds as nYEARS - 1\n  nFolds = len(fold_list) - 1\n  print(f'Total number of folds: {nFolds}')\n  # https://stackoverflow.com/questions/42980704/pyspark-create-new-column-with-mapping-from-a-dict\n  mapping_expr = create_map([lit(str(x)) for x in chain(*mapping.items())])\n  foldedTrainDF = train_df_vec.withColumn('foldCol', mapping_expr[col(fold_variable)].cast('integer')).cache()\n  foldedTestDF = test_df_vec.withColumn('foldCol', mapping_expr[col(fold_variable)].cast('integer')).cache()\n\n  # foldedTrainDF = foldedTrainDF.withColumnRenamed(('DEP_DEL15', 'label_cat'), ('DEP_DEL15', 'label_cont')).cache()\n  foldedTrainDF = foldedTrainDF.filter(col('DEP_DEL15').isNotNull()).select(label_cat, label_cont, feature_col, 'foldCol').cache()\n  foldedTestDF = foldedTestDF.filter(col('DEP_DEL15').isNotNull()).select(label_cat, label_cont, feature_col).cache()\n\n  train_df_vec.unpersist()\n  \n  foldedTrainDF.count()\n  foldedTrainDF.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{STAGE}/foldedTrainDF-{model}_features\")\n  foldedTestDF.count()\n  foldedTestDF.write.mode(\"overwrite\").parquet(f\"{blob_url}/processed-{STAGE}/foldedTestDF-{model}_features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8c47f87-7e98-4218-81a7-4af371b9bfa6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_cvdata(env='dev', features='dev'):\n  if features == 'prod':\n    model = \"pca\"\n  else:\n    model = features\n  foldedTrainDF = spark.read.parquet(f\"{blob_url}/processed-{STAGE}/foldedTrainDF-{model}_features\").cache()\n  foldedTestDF = spark.read.parquet(f\"{blob_url}/processed-{STAGE}/foldedTestDF-{model}_features\").cache()\n  return foldedTrainDF, foldedTestDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31848322-01f5-4ef3-96df-6639977b59a7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_cvdata(train_data, test_data, env='dev', features='dev', write=False):\n  \"\"\"\n  Returns train and test split dataframes\n  \"\"\"\n  STAGE = env\n  FEATURES = features\n  if write == True:\n    write_cvdata(train_data, test_data, env=STAGE, features=FEATURES)\n  else:\n    pass\n  train_df, test_df =  read_cvdata(env=STAGE, features=FEATURES)\n  print(f\"Your new training set has {train_df.count():,} rows.\")\n  print(f\"Your new test set has {test_df.count():,} rows.\")\n  return train_df, test_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85b5207e-cdf4-49ab-ba79-d79f15e3888b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class TimeSeriesCrossValidator(CrossValidator):\n    '''\n    Customizes CrossValidator to perform time series cross validation on a rolling basis.\n    User needs to provide `foldCol` with the fold numbers defined in a time ascending order\n    (e.g. 2015 is assigned as fold 0, 2016 as fold 1, and so on).\n    '''\n    def _kFold(self, dataset):\n        nFolds = self.getOrDefault(self.numFolds)\n        foldCol = self.getOrDefault(self.foldCol)\n\n        datasets = []\n        if not foldCol:\n            # Do random k-fold split.\n            seed = self.getOrDefault(self.seed)\n            h = 1.0 / nFolds\n            randCol = self.uid + \"_rand\"\n            df = dataset.select(\"*\", rand(seed).alias(randCol))\n            for i in range(nFolds):\n                validateLB = i * h\n                validateUB = (i + 1) * h\n                condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n                validation = df.filter(condition)\n                train = df.filter(~condition)\n                datasets.append((train, validation))\n        else:\n            # Use user-specified fold numbers.\n            def checker(foldNum):\n                if foldNum < 0 or foldNum > nFolds:\n                    raise ValueError(\n                        \"Fold number must be in range [0, %s], but got %s.\" % (nFolds, foldNum)\n                    )\n                return True\n\n            checker_udf = UserDefinedFunction(checker, BooleanType())\n            for i in range(nFolds):\n                training = dataset.filter(checker_udf(dataset[foldCol]) & (col(foldCol) <= lit(i))) # Training set always in the past\n                validation = dataset.filter(\n                    checker_udf(dataset[foldCol]) & (col(foldCol) == lit(i+1)) # Validation set always in the future\n                )\n                if training.rdd.getNumPartitions() == 0 or len(training.take(1)) == 0:\n                    raise ValueError(\"The training data at fold %s is empty.\" % i)\n                if validation.rdd.getNumPartitions() == 0 or len(validation.take(1)) == 0:\n                    raise ValueError(\"The validation data at fold %s is empty.\" % i)\n                datasets.append((training, validation))\n\n        return datasets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dc47450-b9d8-4c48-b8f9-579286e17ef4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def apply_pca(env='dev'):\n  # Apply PCA\n  pca = PCA(k=100, inputCol=\"scaledFeatures\")\n  pca.setOutputCol(\"pcaFeatures\")\n  model_pca = pca.fit(transformed_data.filter('YEAR < 2019'))\n  pca_data = model_model.transform(transformed_data).select(*['DEP_DEL15', 'DEP_DELAY_NEW', \"YEAR\", \"pcaFeatures\"])\n  pca_data.write.mode('overwrite').parquet(f\"{blob_url}/feature-engineering/pca-data\")\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"833e6cf2-1051-4cc9-b6a1-a5fc33d04784"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"helper_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1898361324239619}},"nbformat":4,"nbformat_minor":0}
